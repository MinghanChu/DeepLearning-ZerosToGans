{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAhcdc1xFP2k4xM6FZLV7E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinghanChu/DeepLearning-ZerosToGans/blob/main/Data_Augmentation%2C_Regularization_%26_ResNets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "til2fY5-zGky"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_name='05b-cifar10-resnet'"
      ],
      "metadata": {
        "id": "rJyUzy7Jz31Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "\n",
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "# Extract from archive\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "\n",
        "# Look into the data directory\n",
        "data_dir = './data/cifar10'\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql_V7c_-z7ox",
        "outputId": "de2fb5ae-a23b-4498-e88a-c8093fb293ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz to ./cifar10.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 135107811/135107811 [00:02<00:00, 49048327.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test', 'train']\n",
            "['ship', 'automobile', 'truck', 'dog', 'deer', 'bird', 'horse', 'frog', 'airplane', 'cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create **training and validation datasets** using the `ImageFolder` class from `torchvision`. In addition to the `ToTensor` transform, we'll also apply some other transforms to the images. There are a few important changes we'll make while creating PyTorch datasets for training and validation:\n",
        "\n",
        "1. **Use test set for validation**: Instead of setting aside a fraction (e.g. 10%) of the data from the training set for validation, we'll **simply use the test set as our validation set**. This just gives **a little more data** to train with. In general, once you have picked the best model architecture & hypeparameters using a fixed validation set, it is a good idea to retrain the same model on the entire dataset just to give it a small final boost in performance.\n",
        "2. **Channel-wise data normalization**: We will **normalize the image tensors** by **subtracting the mean and dividing by the standard deviation** across each channel. As a result, the mean of the data across each channel is 0, and standard deviation is 1. Normalizing the data prevents the values from any one channel from disproportionately affecting the losses and gradients while training, simply by having a higher or wider range of values that others.\n",
        "\n",
        "<img src=\"https://i.imgur.com/LYxXBVg.png\" width=\"360\">\n",
        "\n",
        "\n",
        "3. **Randomized data augmentations**: We will apply randomly chosen transformations while loading images from the training dataset. Specifically, we will **pad each image by 4 pixels**, and then take a random crop of size 32 x 32 pixels, and then flip the image horizontally with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the **model sees slightly different images in each epoch of training**, which allows it generalize better.\n",
        "\n",
        "![data-augmentation](https://imgaug.readthedocs.io/en/latest/_images/cropandpad_percent.jpg)We can create training and validation datasets using the `ImageFolder` class from `torchvision`. In addition to the `ToTensor` transform, we'll also apply some other transforms to the images. There are a few important changes we'll make while creating PyTorch datasets for training and validation:\n",
        "\n",
        "1. **Use test set for validation**: Instead of setting aside a fraction (e.g. 10%) of the data from the training set for validation, we'll simply use the test set as our validation set. This just gives a little more data to train with. In general, once you have picked the best model architecture & hypeparameters using a fixed validation set, it is a good idea to retrain the same model on the entire dataset just to give it a small final boost in performance.\n",
        "2. **Channel-wise data normalization**: We will normalize the image tensors by **subtracting the mean and dividing by the standard deviation across each channel (Note here channel does not refer to 3 colors but more features that affect the determination of an image.)**. As a result, the mean of the data across each channel is 0, and standard deviation is 1. Normalizing the data prevents the values from any one channel from disproportionately affecting the losses and gradients while training, simply by having a higher or wider range of values that others.\n",
        "\n",
        "<img src=\"https://i.imgur.com/LYxXBVg.png\" width=\"360\">\n",
        "\n",
        "\n",
        "3. **Randomized data augmentations**: We will apply randomly chosen transformations while loading images from the training dataset. Specifically, we will pad each image by 4 pixels, and then take a random crop of size 32 x 32 pixels, and then flip the image horizontally with a 50% probability. Since the transformation will be applied randomly and dynamically each time a particular image is loaded, the model sees slightly different images in each epoch of training, which allows it generalize better.\n",
        "\n",
        "![data-augmentation](https://imgaug.readthedocs.io/en/latest/_images/cropandpad_percent.jpg)"
      ],
      "metadata": {
        "id": "wzsTuEi30DgZ"
      }
    }
  ]
}