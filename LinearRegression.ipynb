{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRc9NqDpvxQqNj7GdiRblv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Renshui-MC/DeepLearning-ZerosToGans/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gradient Descent and Linear Regression with PyTorch**\n",
        "\n",
        "+ understand what the **linear regression** and **gradient descent** are\n",
        "+ implement a **linear regression model** using `PyTorch`\n",
        "+ train your linear regression model using the **gradient descent algorithm**\n",
        "+ implement gradient descent using `PyTorch` built-in\n",
        "\n",
        "**Linear regression** is one of the foundamental algorithms in machine learning (ML). Most ML courses beigin with linear regression.\n",
        "\n",
        "In a lineear regression model, you have \n",
        "\n",
        "1. **weight** to the **input variable** ($\\vec{x}^{(i)}_j$, where $(i)$ represents training examples and $j$ represents features)\n",
        "2. **target (output) variable** ($y^{(i)}$)\n",
        "3. offset by some constant (bias)\n",
        "\n",
        "```\n",
        "y1  = w11 * feature1 + w12 * feature2 + w13 * feature3 + b1\n",
        "y2  = w21 * feature1 + w22 * feature2 + w23 * feature3 + b2\n",
        "```\n",
        "\n",
        "Note that values of weights vary with targets. `b1` and `b2` are biases that are added to give feasible results, e.g., when all features are at a vlaue of zero we still get predictions for our targets.\n",
        "\n",
        "**Learning** is a process to find the best set of weights `w11, w12, ..., b1 and b2` using **training data**, to accurately predict the new data. One of the fundational learning technique is called **gradient descent**. This technique can be used through the combination of `numpy` and `PyTorch`."
      ],
      "metadata": {
        "id": "XaoDHqGZWE5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "h0DW5fACK-Ae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training data\n",
        "\n",
        "+ two matrices: `inputs` and `targets`\n",
        "+ rows always represents **observation data**\n",
        "+ columns always represent **targets**\n",
        "\n",
        "First we use `numpy` arrays to store our data because `numpy` is a powerful python library to deal with matrices. Then we convert it to `pytorch` tensors for traning. **Floating point numbers** are good for mathematical operations. \n",
        "\n",
        "It is efficient to operate on the input and output variables separatelly. "
      ],
      "metadata": {
        "id": "GFv1tXPIbzuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input variables (multiple features: temp (col1), rainfall (col2), humidity (col3))\n",
        "inputs = np.array([[73, 67, 43], \n",
        "                   [91, 88, 64], \n",
        "                   [87, 134, 58], \n",
        "                   [102, 43, 37], \n",
        "                   [69, 96, 70]], dtype='float32') #you either do 73. or use 'dtype='float32' to convert to floating point"
      ],
      "metadata": {
        "id": "GCEb8Zorjn3F"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70], \n",
        "                    [81, 101], \n",
        "                    [119, 133], \n",
        "                    [22, 37], \n",
        "                    [103, 119]], dtype='float32')"
      ],
      "metadata": {
        "id": "x4Xb7G8ImEFL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert them to `PyTorch` tensors via `torch.from_numpy(variable_name)`."
      ],
      "metadata": {
        "id": "bIKXxIJ63Fju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert inputs and targets to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aworVMVQ3HC5",
        "outputId": "39e3b2de-09dd-41b9-db28-f78a2f0a28f1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a linear regression model\n",
        "\n",
        "Now we have everythin to create a linear regression model. We can beigin with random **weights** `w` and **biases** `b` based on a **normal distribution**."
      ],
      "metadata": {
        "id": "tdI1uYJZ3Wvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2 , 3, requires_grad=True) #two rows and three columns\n",
        "b = torch.randn(2, requires_grad=True) #one row and two columns\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajt0C9O-3i05",
        "outputId": "1b905509-4df1-4dc5-ce64-24ed6d2e1a2b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.7605, -1.8451,  0.8997],\n",
            "        [-0.2172, -0.8063, -0.8036]], requires_grad=True)\n",
            "tensor([ 1.1185, -1.2115], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have\n",
        "\n",
        "+ inputs in pytorch tensor form\n",
        "+ targets in pytorch tensor form\n",
        "+ weights in pytorch tensor form\n",
        "+ biase in pytorch tensor form\n",
        "\n",
        "the model can be created:\n",
        "\n",
        "```\n",
        "model = inputs x weights(transposed) + b \n",
        "```\n",
        "\n",
        "To get the transpose of a matrix, use `.t()` and `@` represents **matrix multiplication.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fObCM7cK5qxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test output here\n",
        "Output = inputs @ w.t() + b\n",
        "print(Output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esTtLNnR7VEc",
        "outputId": "43d877d8-f1fa-42a6-a08c-a9c321cef60d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  44.6983, -105.6436],\n",
            "        [  56.5331, -143.3608],\n",
            "        [ -40.7817, -174.7607],\n",
            "        [ 134.6371,  -87.7695],\n",
            "        [   8.4395, -149.8543]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid repetition, we want to create a function to pass the inputs and predict the targets."
      ],
      "metadata": {
        "id": "wEFUedHf8DFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "  return x @ w.t()+b"
      ],
      "metadata": {
        "id": "iQ_4eGUT6qKo"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's pass the inputs to the created **linear regression model** and predict the target variables."
      ],
      "metadata": {
        "id": "A5tD9Jp-9Kl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict target variables\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6_Qc6Mj9JA5",
        "outputId": "b13a90ce-4c08-4768-a8ee-7607659e182a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  44.6983, -105.6436],\n",
            "        [  56.5331, -143.3608],\n",
            "        [ -40.7817, -174.7607],\n",
            "        [ 134.6371,  -87.7695],\n",
            "        [   8.4395, -149.8543]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know how accurate the model is we need to compare the predictions with actual targets."
      ],
      "metadata": {
        "id": "gZHAKmlN_uRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6sXA9CuAXYQ",
        "outputId": "693a8cb5-870c-40c6-95ae-22bfbc7f212e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison shows that the predictions are quite off from the the targets, becuase the model is based on **random weights and biases**. Therefore, we need to improve the model. \n",
        "\n",
        "+ to evaluate the mean squared error (MSE)\n",
        "\n",
        "```\n",
        "MSE = $Σ[(preds - actual)^2]/#elements$\n",
        "```\n",
        "\n",
        "Note that **MSE** is a **single value**."
      ],
      "metadata": {
        "id": "rR_vYeN_Adj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = preds - targets\n",
        "#To get negative values\n",
        "\n",
        "torch.sum(diff * diff) / diff.numel() #add all entry elements up and divide by the number of elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LPis8WfBVn0",
        "outputId": "5605c3cf-0a36-4289-aa5a-d02f8c671eee"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(32101.4941, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The loss function**\n",
        "To avoid repetition, we create the MSE function. \n",
        "\n",
        "+ `torch.sum` returns the sum of all elements in a matrix\n",
        "+ `.numel()` returns the number of elements in a matrix"
      ],
      "metadata": {
        "id": "6cL9rqbyDem9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE loss\n",
        "def mse(P,T):\n",
        "  diff = P - T\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "MFymd-URDjZn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute MSE using the created model above"
      ],
      "metadata": {
        "id": "zSRHV8ChIw0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRzbMs8MI4r6",
        "outputId": "96df60f7-f3c7-4b1b-8a0a-c3283300decf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(32101.4941, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **MSE** model, it is clear that the **square root of the error** is quite large. It indicates that the model is bad at predicting the target variables. **The lower the loss the better the model.**"
      ],
      "metadata": {
        "id": "zdfC87muJ1Ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**To reduce the loss using gradient descent technique**\n",
        "\n",
        "+  recall we set `requires_grad=True` to `w` and `b`\n",
        "+ loss is a function of `w` and `b` \n",
        "+ compute gradients using `.backward()`\n",
        "+ calculated gradients are stored in `.grad`\n",
        "+ note `grad` can be implicitly created only for **scalar** outputs (loss function outputs a single scalar value)\n",
        "\n",
        "In this particular example, not only are there multiple features (temp (col1), rainfall (col2), humidity (col3)), but also mutliple targets (apples and orange). This indicates that we need two sets of $\\vec{w}_j$. Recall $j$ represents features. Three features are required to describe one target, e.g., $j = 3$. Therefore, we need six weights for two targets. Further, each target needs a cost function to train the weights. "
      ],
      "metadata": {
        "id": "Owcq3hEOKqXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients \n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "CvUs0fDXPhnw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovPcDDuvK0cW",
        "outputId": "924fe71b-0745-4a60-81fa-3f525cb4d7e0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -2397.6421,  -5711.0923,  -2754.1711],\n",
            "        [-18622.2852, -21137.3809, -12895.6318]])\n",
            "tensor([ -35.4947, -224.2778])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a gradient element is positive:\n",
        "\n",
        "+ **increasing the weight** element's value slightly will **increase the loss**\n",
        "+ **decreasing the weight** element's value slightly will **decrease the loss**\n",
        "\n",
        "If a gradient element is negative:\n",
        "\n",
        "+ **increasing the weight** element's value slightly will **decrease the loss**\n",
        "+ **decreasing the weight** element's value slightly will **increase the loss**\n",
        "\n",
        "`PyTorch` will do this judement for us. Now we need to choose an appropriate **learning rate ($α = 1e-5$)** to train the weights. `with torch.no_grad()` disables gradient calculation because we have obtained the gradients via `loss.backward()` and we do not need to recalculate the gradients again.\n",
        "\n",
        "It should be noted that `w.grad` is just taking the derivative of the cost function, i.e., $\\frac{\\partial J(\\vec{w},b)}{\\partial w_{n}}$ where $J$ is the cost function. The following coding reads as:\n",
        "\n",
        "$$w_{n} = w_{n}-α\\frac{\\partial J(\\vec{w},b)}{\\partial w_{n}}$$"
      ],
      "metadata": {
        "id": "ye43nVHDXO3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5"
      ],
      "metadata": {
        "id": "2OQZNtSSNdxG"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With gradient descent technique (updated weights and biases), let's test if the model can give more accurate predictions. \n",
        "\n",
        "Remember to reset the gradients to zero by invoking the `.zero_()` method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
      ],
      "metadata": {
        "id": "IMHaJ9fNSG5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)#use the new weights\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(loss)\n",
        "print(torch.sqrt(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qibMWS2WSQyp",
        "outputId": "c5667bfb-04da-4560-be9e-4ef0fffa6b96"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n",
            "tensor(22938.9590, grad_fn=<DivBackward0>)\n",
            "tensor(151.4561, grad_fn=<SqrtBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Reduce the loss further by training for multiple epochs**\n",
        "\n",
        "Let's train the model for $100$ epochs."
      ],
      "metadata": {
        "id": "ndkGEY0NU2j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(200):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "Na4jZffBVO-s"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify if the loss gets lower:"
      ],
      "metadata": {
        "id": "qxPvwo25VmMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)\n",
        "print(torch.sqrt(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlJRXMTtVryk",
        "outputId": "ea470dd8-6c4b-417a-99d6-0f1f9a907b2f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(129.5387, grad_fn=<DivBackward0>)\n",
            "tensor(11.3815, grad_fn=<SqrtBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare preditions to targets:"
      ],
      "metadata": {
        "id": "3jAZyqEFWUzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSvD2Es8WZKv",
        "outputId": "37ba3394-69bf-4e84-cca6-8e14882cfaee"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 60.8545,  72.1634],\n",
              "        [ 88.6902,  96.7180],\n",
              "        [ 98.0203, 138.9619],\n",
              "        [ 41.8529,  48.4389],\n",
              "        [101.2956, 105.3587]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQGSZcuQWcDJ",
        "outputId": "386e7ae2-6446-4133-e683-f979343c777d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}