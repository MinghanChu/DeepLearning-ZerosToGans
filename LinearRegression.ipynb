{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrp1/YP14arE9IHrRi3Abl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinghanChu/DeepLearning-ZerosToGans/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Supervised (machine) learning**\n",
        "+ input to output mappings\n",
        "+ most economic value created through supervised learning\n",
        "+ **regression model** to predict **numbers** from **infinitely** many possible outputs\n",
        "+ **classification model** to predict categories from **several** possible outputs\n",
        "+ **Supervised** means we know the ''right answers (ground truth)''. For example, we have a dataset that contains the information of house prices and the corresponding house sizes. A supervised regression model is a continous function that can potentially give infinite number of predictions, from which we usually need one or several predictions for our particular applications.\n",
        "\n",
        "\n",
        "Input  | Output | Application\n",
        "-------------------|--------------------|------------------\n",
        "email              | spam?              | spam filtering\n",
        "audio              |text transcripts    | speech recognition\n",
        "English            | Chinese            | machine translation\n",
        "\n",
        "##**Linear Regression and Gradient Descent**\n",
        "\n",
        "+ understand what the **linear regression** and **gradient descent** are\n",
        "+ implement a **linear regression model** using `PyTorch`\n",
        "+ train your linear regression model using the **gradient descent algorithm**\n",
        "+ implement gradient descent using `PyTorch` built-in\n",
        "\n",
        "**Linear regression** is one of the foundamental algorithms in machine learning (ML). Most ML courses beigin with linear regression. Use a data table to help us understand what a linear regression model requires:\n",
        "\n",
        "+ **training set** is a dataset used to train your model, denoted $x =$ **input variable (feature)**\n",
        "+ **output variable (target) variable** is the dataset to predict, denoted $y$\n",
        "+ $m$ number of training examples ($m = 47$ in the following example), e.g., $(x,y)= (2104,400)$ (refer to rows)\n",
        "+ $(x^{(i)}_{j}, y^{(i)}_{j}) = i^{th}$ training example, $i$ refers to specific rows in this example\n",
        "\n",
        "\\\\\n",
        "\n",
        "*superscript for $i$th row (training example) and subscript for $j th$ column (feature)*\n",
        "\n",
        "\\\\\n",
        "\n",
        "size ($x$)         | price ($y$)|\n",
        "-------------------|--------------------\n",
        "(1) 2104              | 400              \n",
        "(2) 1416              | 232   \n",
        "(3) 1534              | 315            \n",
        "...                   | ...\n",
        "(47) 3210             | 870\n",
        "\n",
        "In a training set, you have features ($x^{(i)}_{j}$) and targets ($y^{(i)}_{j}$). On the other hand, a regression model takes features and gives predictions.\n",
        "\n",
        "+ a regression model denoted $f$\n",
        "+ the model takes input features $x^{(i)}_{j}$\n",
        "+ the model gives prediction $\\hat{y}^{(i)}$. (**Note that $y^{(i)}_{j}$ refers to the target or actual true value in a training set, while $\\hat{y}^{(i)}$ refers to the prediction**)\n",
        "+ if we have only **one input feature variable**, we call the $f$ the **univariate linear regression** (In practice, we usually have multiple input features)\n",
        "\n",
        "\\\\\n",
        "\n",
        "###**Linear regression**\n",
        "\n",
        "A linear regression model can be defined as follows:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = wx^{(i)} + b,\n",
        "$$\n",
        "\n",
        "where $w,b$ are called **parameters** or **weights.** The **purpose** is to find $w,b$ that can give $\\hat{y}^{(i)}$ that is close to $y^{(i)}$.\n",
        "\n",
        "###**Cost function (sqaured error cost function)**\n",
        "Depending on the values of weights, you get different $f$ functions. We need a **cost function** to measure how well $f$ is fitted to the training data. The cost function basically measures the **difference between the prediction and the actual true value:**\n",
        "\n",
        "$$J(w, b)=\\frac{1}{2 m} \\sum_{i=1}^m\\left(f_{w, b}\\left(x^{(i)}\\right)-y^{(i)}\\right)^2. $$\n",
        "\n",
        "The purpose of machine learning is to find the values of $w,b$ that can **minimize** the cost function. In math, it reads as\n",
        "\n",
        "$$\n",
        "\\operatorname{minimize}_{w, b} J(w, b).\n",
        "$$\n",
        "\n",
        "The above equation indicates that we want to minimize $J$ as a function of $w,b$. For different values of $w,b$, you can trace out what the cost function $J$ looks like. Later we can use **gradient descent technique** to find the minimum value of $J$, and hence, determine the corresponding values of $w,b$.\n",
        "\n",
        "###**Gradient descent**\n",
        "\n",
        "We start off with some initial guesses for $w,b$, e.g., $w=0,b=0$. Keep changing the values of $w,b$ to reduce $J(w,b)$ until it approaches or near minimum.\n",
        "\n",
        "$$w = w -α\\frac{\\partial J(w,b)}{\\partial w}$$\n",
        "$$b = b -α\\frac{∂J(w,b)}{∂b}.$$\n",
        "\n",
        "$α$ is the **learning rate** that controls how big the step is. A small value of $α$ will give you a small step change in reducing $w$. **Very importantly, we always update $w$ and $b$ simultaneously.**\n",
        "\n",
        "1. **weight** to the **input variable** ($\\vec{x}^{(i)}_j$, where $(i)$ represents training examples and $j$ represents features)\n",
        "2. **target (output) variable** ($y^{(i)}$)\n",
        "3. note that $y^{(i)}$ has only superscript to indicate training examples (no need to add subscript to indicate features)\n",
        "4. $b$ stands for offset by some constant (bias)\n",
        "\n",
        "+ gradient descent reduces $w$ when $J > 0$, which shifts $J$ toward the minimum\n",
        "+ gradient descent increases $w$ when $J < 0$, which also shifts $J$ toward the minimum\n",
        "\n",
        "###**Multiple features**\n",
        "So far we only deal with one feature variable, i.e., input $x^{(i)}$ = size. In practice, we always have multiple features: $x^{(i)}_{j}$. Then the regression model can be expressed as\n",
        "\n",
        "$$\n",
        "f_{\\overrightarrow{\\mathrm{w}}, b}(\\overrightarrow{\\mathrm{x}})=\\overrightarrow{\\mathrm{w}} \\cdot \\overrightarrow{\\mathrm{x}}+b,\n",
        "$$\n",
        "\n",
        "where $\\vec{x} = [x_1 \\ x_2 \\ x_3 \\ ... \\ x_n]$ and $\\vec{w} = [w_1 \\ w_2 \\ w_3 \\ ... \\ w_n]$.  \n",
        "\n",
        "\\\\\n",
        "\n",
        "###**Gradient descent for multiple linear regression**\n",
        "\n",
        "The cost function for multiple linear regression can be expressed as follows:\n",
        "\n",
        "$$J(\\vec{w},b).$$\n",
        "\n",
        "and the gradient descent can be defined written as follows:\n",
        "\n",
        "$$\n",
        "w_j=w_j-\\alpha \\frac{\\partial}{\\partial w_j} J\\left(w_1, \\cdots, w_n, b\\right),\n",
        "$$\n",
        "\n",
        "$$\n",
        "b=b-\\alpha \\frac{\\partial}{\\partial b} J\\left(w_1, \\cdots, w_n, b\\right).\n",
        "$$\n",
        "\n",
        "+ note $w_{j}$ only varies with input features\n",
        "\n",
        "Now let's take derivative with respect to $w_1$ at $j=1$ or feature one:\n",
        "\n",
        "$$\n",
        "w_1=w_1-\\alpha \\frac{\\partial}{\\partial w_1} J(\\overrightarrow{\\mathrm{w}}, b) = \\sum_{i=1}^m\\left(f_{\\overrightarrow{\\mathrm{w}, b}}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)-y^{(i)}\\right) x_1^{(i)}.\n",
        "$$\n",
        "\n",
        "Similarly, we can take derivative with respect to any feature at $j = n$:\n",
        "\n",
        "$$\n",
        "w_n=w_n-\\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)-y^{(i)}\\right) x_n^{(i)},\n",
        "$$\n",
        "\n",
        "$$\n",
        "b=b-\\alpha \\frac{1}{m} \\sum_{i=1}^m\\left(f_{\\overrightarrow{\\mathrm{w}}, b}\\left(\\overrightarrow{\\mathrm{x}}^{(i)}\\right)-y^{(i)}\\right).\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "In addition, you can have multiple targets to predict. In that case, you will have multiple sets of weights (one set has the weights at the same number of the features). For example, the regression model is used to predict two targets:\n",
        "\n",
        "\n",
        "$$\\hat{y1}^{(i)}  = w_{11} \\times x_{1}^{(i)} + w_{12} \\times x_{2}^{(i)} + w_{13} \\times x_{3}^{(i)} + b1 $$\n",
        "$$\\hat{y2}^{(i)}  = w_{21} \\times x_{1}^{(i)} + w_{22} \\times x_{2}^{(i)} + w_{23} \\times x_{3}^{(i)} + b2$$\n",
        "\n",
        "Note that the above equations are the results of multiplying matrices ($Ax=b$). The regression model can be used to train the **two sets of weights** for each target. Further, weights `b1` and `b2` are also called **biases** that are added to ensure the predictions are realizable, e.g., when all features are at a value of zero we still get predictions for our targets.\n",
        "\n",
        "\\\\\n",
        "\n",
        "##**Goal**\n",
        "**Learning** is a process to find the best set of weights `w11, w12, ..., b1 and b2` using **training data**, to accurately predict the new data that are close to the targets. One of the fundational learning technique is called **gradient descent**. This technique can be used through the combination of `numpy` and `PyTorch`.\n"
      ],
      "metadata": {
        "id": "XaoDHqGZWE5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0DW5fACK-Ae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training data\n",
        "\n",
        "+ two matrices: `inputs` and `targets`\n",
        "\n",
        "First we use `numpy` arrays to store our data because `numpy` is a powerful python library to deal with matrices. Then we convert it to `pytorch` tensors for traning. **Floating point numbers** are good for mathematical operations.\n",
        "\n",
        "It is efficient to operate on the input and output variables separatelly."
      ],
      "metadata": {
        "id": "GFv1tXPIbzuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input variables (multiple features: temp (col1), rainfall (col2), humidity (col3))\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32') #you either do 73. or use 'dtype='float32' to convert to floating point"
      ],
      "metadata": {
        "id": "GCEb8Zorjn3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Target variable__ is like the estimated _posterior state_ in `Kalman filter`. __Input variable__ is like the _prior state_ from the prediction model. The __bias__ is like the noise/uncertainty added to the prior state."
      ],
      "metadata": {
        "id": "97NBs4W36kgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert them to `PyTorch` tensors via `torch.from_numpy(variable_name)`."
      ],
      "metadata": {
        "id": "bIKXxIJ63Fju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Targets (apples, oranges)\n",
        "targets = np.array([[56, 70],\n",
        "                    [81, 101],\n",
        "                    [119, 133],\n",
        "                    [22, 37],\n",
        "                    [103, 119]], dtype='float32')"
      ],
      "metadata": {
        "id": "x4Xb7G8ImEFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert inputs and targets to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aworVMVQ3HC5",
        "outputId": "094cd61f-4c24-409b-f8f6-a839825f2b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a linear regression model\n",
        "\n",
        "Now we have everything to create a linear regression model. We can beigin with random **weights** `w` and **biases** `b` based on a **normal distribution**."
      ],
      "metadata": {
        "id": "tdI1uYJZ3Wvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2 , 3, requires_grad=True) #two rows and three columns\n",
        "b = torch.randn(2, requires_grad=True) #one row and two columns\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajt0C9O-3i05",
        "outputId": "7243f43d-d04a-4942-96b1-c9398dd5daab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3349, -1.3759, -0.0413],\n",
            "        [ 0.7914,  0.9177, -0.6042]], requires_grad=True)\n",
            "tensor([1.9568, 1.1380], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have\n",
        "\n",
        "+ inputs in pytorch tensor form\n",
        "+ targets in pytorch tensor form\n",
        "+ weights in pytorch tensor form\n",
        "+ biase in pytorch tensor form\n",
        "\n",
        "the model can be created:\n",
        "\n",
        "```\n",
        "model = inputs x weights(transposed) + b\n",
        "```\n",
        "\n",
        "To get the transpose of a matrix, use `.t()` and `@` represents **matrix multiplication.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fObCM7cK5qxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test output here\n",
        "Output = inputs @ w.t() + b\n",
        "print(Output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esTtLNnR7VEc",
        "outputId": "681f92e2-76a0-4448-95eb-83e859c7f3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5.4423,  94.4197],\n",
            "        [ -0.2909, 115.2500],\n",
            "        [-68.6733, 157.9256],\n",
            "        [ 77.4226,  98.9692],\n",
            "        [-40.9130, 101.5564]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid repetition, we want to create a function to pass the inputs and predict the targets."
      ],
      "metadata": {
        "id": "wEFUedHf8DFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "  return x @ w.t()+b"
      ],
      "metadata": {
        "id": "iQ_4eGUT6qKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's pass the inputs to the created **linear regression model** and predict the target variables."
      ],
      "metadata": {
        "id": "A5tD9Jp-9Kl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict target variables\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6_Qc6Mj9JA5",
        "outputId": "7eab8144-3602-4ce6-b5e4-80a1357412a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  5.4423,  94.4197],\n",
            "        [ -0.2909, 115.2500],\n",
            "        [-68.6733, 157.9256],\n",
            "        [ 77.4226,  98.9692],\n",
            "        [-40.9130, 101.5564]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know how accurate the model is we need to compare the predictions with __actual targets__. Here we have 10 targets."
      ],
      "metadata": {
        "id": "gZHAKmlN_uRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6sXA9CuAXYQ",
        "outputId": "bfa9f5f8-b1b2-4a0d-ec83-89c4ae247650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 133.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison shows that the predictions are quite off from the the targets, becuase the model is based on **random weights and biases**. Therefore, we need to improve the model.\n",
        "\n",
        "+ to evaluate the mean squared error (MSE) See Eqn. 11\n",
        "\n",
        "```\n",
        "MSE = $Σ[(preds - actual)^2]/#elements$ see Eqn.\n",
        "```\n",
        "+ calculate the difference between the two matrices (`preds` and `targets`). (This step is like calculating the residual in _Kalman filter_.)\n",
        "\n",
        "+ square all elements of the difference matrix to remove negative values.\n",
        "\n",
        "+ calculate the average of the elements in the resulting matrix.\n",
        "\n",
        "Note that **MSE** is a **single value**."
      ],
      "metadata": {
        "id": "rR_vYeN_Adj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diff = preds - targets\n",
        "#To get negative values\n",
        "\n",
        "torch.sum(diff * diff) / diff.numel() #add all entry elements up and divide by the number of elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LPis8WfBVn0",
        "outputId": "c16ce136-6e7d-4871-c081-7efb6cef3dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7373.3345, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The cost function**\n",
        "To avoid repetition, we create the MSE function.\n",
        "\n",
        "+ `torch.sum` returns the sum of all elements in a matrix\n",
        "+ `.numel()` returns the number of elements in a matrix"
      ],
      "metadata": {
        "id": "6cL9rqbyDem9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE loss\n",
        "def mse(P,T):\n",
        "  diff = P - T\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "MFymd-URDjZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compute MSE using the created model above"
      ],
      "metadata": {
        "id": "zSRHV8ChIw0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRzbMs8MI4r6",
        "outputId": "b7ae388d-42ee-4e37-c0eb-9bcd8088125e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(7373.3345, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **MSE** model, it is clear that the **square root of the error** is quite large. It indicates that the model is bad at predicting the target variables. **The lower the loss the better the model.**"
      ],
      "metadata": {
        "id": "zdfC87muJ1Ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**To reduce the loss using gradient descent technique**\n",
        "\n",
        "+  recall we set `requires_grad=True` to `w` and `b`\n",
        "+ loss is a function of `w` and `b`\n",
        "+ compute gradients using `.backward()`\n",
        "+ calculated gradients are stored in `.grad`\n",
        "+ note `grad` can be implicitly created only for **scalar** outputs (loss function outputs a single scalar value)\n",
        "\n",
        "In this particular example, not only are there multiple features (temp (col1), rainfall (col2), humidity (col3)), but also mutliple targets (apples and orange). This indicates that we need two sets of $\\vec{w}_j$. Recall $j$ represents features. Three features are required to describe one target, e.g., $j = 3$. Therefore, we need six weights for two targets. Further, each target needs a cost function to train the weights."
      ],
      "metadata": {
        "id": "Owcq3hEOKqXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute gradients\n",
        "`Pytorch` can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases because they have `requires_grad` set to `True`."
      ],
      "metadata": {
        "id": "44t5pAmxMIqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "CvUs0fDXPhnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovPcDDuvK0cW",
        "outputId": "4faee347-1412-4c9c-9036-83c9368ec976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-6338.5322, -9424.3359, -5256.9863],\n",
            "        [ 2073.0339,  1444.0497,   895.9088]])\n",
            "tensor([-81.6025,  21.6242])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adjust weights and biases to reduce the loss vs Kalman gain in Kalman filter**\n",
        "\n",
        "Our objective is to find the set of weights where the loss is the lowest.\n",
        "\n",
        "In _Kalman filter_, Kalman gain serves as the weight in machine learning. It should be also used to minimize the loss. between prediction and measurement. However, the _Kalman filter_ is not a gradient-based method. Instead, it's a **recursive algorithm** that estimates the state of a dynamic system from a series of noisy measurements.\n",
        "\n",
        "The _Kalman filter_ operates by iteratively updating its estimates of the state based on new measurements and the system's dynamics model.\n",
        "\n",
        "Therefore, instead of adjusting the Kalman gain through taking the derivative of a loss function w.r.t. Kalman gain, we obtain optimal results with the Kalman filter by selecting an __appropriate initial covariance matrix__ for the state estimate!! (This is the key difference between linear regression and Kalman filter.)\n",
        "\n",
        "The _covariance matrix_ represents the uncertainty or error in the initial state estimate. By choosing an initial covariance matrix that accurately reflects the uncertainty in the initial state, the Kalman filter can effectively balance between trusting the initial estimate and incorporating new measurements.\n",
        "\n",
        "If the initial covariance matrix underestimates the uncertainty in the initial state, the filter may overly trust the initial estimate and __be slow to adjust to new measurements__, leading to __suboptimal results__. On the other hand, if the initial covariance matrix overestimates the uncertainty, the filter may be __overly cautious__ and __converge more slowly__ to the true state, also leading to __suboptimal performance__.\n"
      ],
      "metadata": {
        "id": "-n60ETBvOmDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a gradient element is positive:\n",
        "\n",
        "+ **increasing the weight** element's value slightly will **increase the loss**\n",
        "+ **decreasing the weight** element's value slightly will **decrease the loss**\n",
        "\n",
        "If a gradient element is negative:\n",
        "\n",
        "+ **increasing the weight** element's value slightly will **decrease the loss**\n",
        "+ **decreasing the weight** element's value slightly will **increase the loss**\n",
        "\n",
        "`PyTorch` will do this judement for us. Now we need to choose an appropriate **learning rate ($α = 1e-5$)** to train the weights.\n",
        "\n",
        "+ learning rate is used to help us avoid modifying the weights by a very large amount.  (It is kind of like choosing an appropriate initial covariance matrix: excessive trust can lead to slow convergence, whereas insufficient trust can cause erratic fluctuations and delay the convergence rate.)\n",
        "\n",
        " `with torch.no_grad()` disables gradient calculation because we have obtained the gradients via `loss.backward()` and we do not need to recalculate the gradients again.\n",
        "\n",
        "It should be noted that `w.grad` is just taking the derivative of the cost function, i.e., $\\frac{\\partial J(\\vec{w},b)}{\\partial w_{n}}$ where $J$ is the cost function. The following coding reads as:\n",
        "\n",
        "$$w_{n} = w_{n}-α\\frac{\\partial J(\\vec{w},b)}{\\partial w_{n}}$$\n",
        "\n",
        "The gradient measures the rate of change of the loss. We perform a simple subtraction of the gradient from the weight. The use of the __'minus' sign__ aligns with the following principles: when the gradient is negative, we increase the weight value to decrease the loss; conversely, when the gradient is positive, we decrease the weight to reduce the loss. The negative sign ensures that this desired outcome is achieved!"
      ],
      "metadata": {
        "id": "ye43nVHDXO3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5"
      ],
      "metadata": {
        "id": "2OQZNtSSNdxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "w.grad * 1e-5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l7sviLacgkq",
        "outputId": "28a673a8-8747-4892-cbb2-8ff3d132a52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.3983, -1.2816,  0.0113],\n",
            "        [ 0.7707,  0.9033, -0.6131]], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0634, -0.0942, -0.0526],\n",
              "        [ 0.0207,  0.0144,  0.0090]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With gradient descent technique (updated weights and biases), let's test if the model can give more accurate predictions.\n",
        "\n",
        "Remember to reset the gradients to zero by invoking the `.zero_()` method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke `.backward` on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
      ],
      "metadata": {
        "id": "IMHaJ9fNSG5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)#use the new weights\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(loss)\n",
        "print(torch.sqrt(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qibMWS2WSQyp",
        "outputId": "d2d76823-cd6d-4550-eee9-d7cd228279b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n",
            "tensor(5878.7480, grad_fn=<DivBackward0>)\n",
            "tensor(76.6730, grad_fn=<SqrtBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the model using gradient descent**\n",
        "\n",
        "In machine learning, linear regression is implemented as a gradient descent optimization algorithm. It adjusts the weights by computing their gradients (derivatives with respect to the weights) to minimize the loss (residual). We can train the model using the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "2. Calculate the loss\n",
        "3. Compute gradients w.r.t. the weights and biases\n",
        "4. Adjust the weights by substracting a small quantity proportional to the gradient (A small quantity results from multiplying the gradient by a learning rate. It's worth noting that the gradient value is usually quite large.)"
      ],
      "metadata": {
        "id": "1UU_Kj_GJMJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "IuA_6x8zN5tw",
        "outputId": "00cde2aa-f65f-4bfe-8d78-33cc2b26c1c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 18.6450,  91.5534],\n",
            "        [ 17.1358, 111.5192],\n",
            "        [-47.4803, 153.6673],\n",
            "        [ 89.8863,  95.9020],\n",
            "        [-23.8114,  98.1124]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the loss\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "EOwBPRsYMge8",
        "outputId": "6a51ad7f-5d9b-43a0-ddec-1abba8e343d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5878.7480, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute gradients\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "IqZfHx76OSq9",
        "outputId": "7b4e3b40-39a0-4be0-a0d4-9c61f39a34f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-14908.7520, -23811.5840, -13028.6582],\n",
            "        [  5337.2754,   3400.0527,   2109.5806]])\n",
            "tensor([-195.9747,   54.4526])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "update the weights and biases using the gradients computed above\n",
        "\n",
        "`torch.no_grad()` context manager ensures that gradients aren't tracked during the forward pass, which is usually desired during inference or evaluation. This can help save memory and speed up computation because the computational graph doesn't need to be stored for gradient calculation."
      ],
      "metadata": {
        "id": "DLT4D-CYPf7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "metadata": {
        "id": "zfSy0DA_PoDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check updated weights and biases"
      ],
      "metadata": {
        "id": "cBJkIRx0R8bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "EySy1GVcR7rv",
        "outputId": "bb195370-8030-4347-b154-1883e84217b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5473, -1.0435,  0.1416],\n",
            "        [ 0.7173,  0.8693, -0.6342]], requires_grad=True)\n",
            "tensor([1.9596, 1.1373], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we expect a significant reduction in the loss with the new weights"
      ],
      "metadata": {
        "id": "icB2hmPCTCof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "m5WVcrecSvQ3",
        "outputId": "122f59d7-b837-499f-d696-63d0b17762a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3405.1355, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen a reduction in the loss by adjusting the weights using gradient descent."
      ],
      "metadata": {
        "id": "tYgEyg9BTW7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Reduce the loss further by training for multiple epochs**\n",
        "\n",
        "Let's train the model for $200$ epochs."
      ],
      "metadata": {
        "id": "ndkGEY0NU2j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for 100 epochs\n",
        "for i in range(100):\n",
        "    preds = model(inputs)\n",
        "    loss = mse(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "Na4jZffBVO-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify if the loss gets lower:\n",
        "\n",
        "Please note that the loss is computed as the sum of squared differences divided by the number of elements. Therefore, computing the square root of the loss is essential for accurately assessing the performance of our linear regression model."
      ],
      "metadata": {
        "id": "qxPvwo25VmMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate loss\n",
        "preds = model(inputs)\n",
        "loss = mse(preds, targets)\n",
        "print(loss)\n",
        "print(torch.sqrt(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlJRXMTtVryk",
        "outputId": "4a017102-e413-4697-835e-d2331c22948e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(845.6121, grad_fn=<DivBackward0>)\n",
            "tensor(29.0794, grad_fn=<SqrtBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare preditions to targets:"
      ],
      "metadata": {
        "id": "3jAZyqEFWUzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSvD2Es8WZKv",
        "outputId": "cf764386-97af-47ed-cbaf-44886af22d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 69.1629,  75.2402],\n",
              "        [ 89.4368,  93.0396],\n",
              "        [ 82.9678, 142.3888],\n",
              "        [ 88.8255,  63.4678],\n",
              "        [ 75.3508,  90.6502]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQGSZcuQWcDJ",
        "outputId": "31aecef8-6f30-49cc-f839-7f23a6291b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 133.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison shows a significant improvment in predicting the target variables."
      ],
      "metadata": {
        "id": "w8DIpVHMnicc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear regression using PyTorch built-ins**"
      ],
      "metadata": {
        "id": "VW5FB4fsAahL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As linear regression and gradient descent model use basic tensor operations, whcih is a common pattern in deep learning. PyTorch provides **built-in** functions and classes to make it easy to create and train models with just a few lines of code.\n",
        "\n",
        "For example:\n",
        "\n",
        "```\n",
        "torch.nn\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "naTzHklkAy8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "5i5QTnJ0AnWp"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}